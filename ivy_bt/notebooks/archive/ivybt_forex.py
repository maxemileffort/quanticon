# -*- coding: utf-8 -*-
"""IvyBT - Forex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/165-7am0hcxrvnfSX9VKUSGP9lp2w6n_H
"""

# try:
#     import pandas_ta as ta
# except ImportError:
#     !pip install pandas-ta --quiet

#     import os
#     os.kill(os.getpid(), 9)

import yfinance as yf
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicomp import MultiComparison
from datetime import datetime
import pandas_ta as ta

import plotly.express as px
from sklearn.ensemble import RandomForestRegressor

# Helpers

def ta_crossover(s1:pd.Series,s2:pd.Series):
    was_under = s1.shift(1) < s2
    now_over = s1 > s2
    return (now_over) & (was_under)

def ta_crossunder(s1:pd.Series,s2:pd.Series):
    was_over = s1.shift(1) > s2
    now_under = s1 < s2
    return (now_under) & (was_over)

def analyze_complex_grid(grid_df, target_metric='Sharpe'):
    """
    Visualizes high-dimensional grid search results.
    """
    # 1. Parallel Coordinates Plot
    # This shows how paths through parameters lead to high/low returns
    fig = px.parallel_coordinates(
        grid_df,
        color=target_metric,
        color_continuous_scale=px.colors.diverging.Tealrose,
        title=f"Multi-Dimensional Strategy Optimization ({target_metric})"
    )
    fig.show()

    # 2. Parameter Importance Logic
    # We use a Random Forest to see which inputs actually 'drive' the Sharpe ratio
    X = grid_df.drop(columns=['Sharpe', 'Return'])
    y = grid_df['Sharpe']

    model = RandomForestRegressor(n_estimators=100)
    model.fit(X, y)

    importance_df = pd.DataFrame({
        'Feature': X.columns,
        'Importance': model.feature_importances_
    }).sort_values(by='Importance', ascending=False)

    # 3. Plot Importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
    plt.title("Which Parameters Actually Matter?")
    plt.show()

    return importance_df

# Example Usage:
# grid_results = engine.run_grid_search(Newsom10Strategy, param_grid)
# importance = analyze_complex_grid(grid_results)

"""## Backtest class"""

# Backtest Class Definition

class BacktestEngine:
    def __init__(self
                 , tickers
                 , start_date
                 , end_date=datetime.today().strftime('%Y-%m-%d')
                 , benchmark='SPY'):
        self.tickers = tickers
        self.start_date = start_date
        self.end_date = end_date
        self.benchmark_ticker = benchmark
        self.data = {}
        self.results = {}
        self.benchmark_data = None
        self.strat_name = 'MyStrategy'

    def fetch_data(self):
        """Downloads data for assets and the benchmark."""
        if isinstance(self.tickers, str):
            self.tickers = [self.tickers]
        df = yf.download(self.tickers, start=self.start_date, end=self.end_date, group_by='ticker')


        for ticker in self.tickers:
          sub_df = df[ticker]
          if not sub_df.empty:
            sub_df.columns = [col[0].lower() if isinstance(col, tuple) else col.lower() for col in sub_df.columns]
            self.data[ticker] = sub_df

        # Fetch Benchmark
        bench = yf.download(self.benchmark_ticker, start=self.start_date, end=self.end_date)
        bench.columns = [col[0].lower() if isinstance(col, tuple) else col.lower() for col in bench.columns]
        bench['log_return'] = np.log(bench['close'] / bench['close'].shift(1))
        bench['signal'] = 1
        bench['position'] = 1
        bench['strategy_return'] = bench['position'] * bench['log_return']
        self.benchmark_data = bench

    def run_strategy(self, strategy_logic, name=None):
        """
        Runs the strategy and stores metrics.
        Accepts either a strategy function or a StrategyTemplate class instance.
        """
        # Determine the strategy name automatically if not provided
        if name:
            self.strat_name = name
        elif hasattr(strategy_logic, 'name'):
            self.strat_name = strategy_logic.name
        else:
            self.strat_name = 'MyStrategy'

        for ticker, df in self.data.items():
            # CHECK: If it's a class instance, call .strat_apply(). Otherwise, call it as a function.
            if hasattr(strategy_logic, 'strat_apply'):
                df = strategy_logic.strat_apply(df)
            else:
                df = strategy_logic(df)

            # Vectorized return calculations
            df['position'] = df['signal'].shift(1).fillna(0)
            df['log_return'] = np.log(df['close'] / df['close'].shift(1))
            df['strategy_return'] = df['position'] * df['log_return']

            self.data[ticker] = df
            self.results[ticker] = self.calculate_metrics(df)

        # Calculate Benchmark Metrics
        self.results[f"BENCHMARK ({self.benchmark_ticker})"] = self.calculate_metrics(self.benchmark_data)

    def calculate_metrics(self, df, slippage=0.001):
        """
        Calculates metrics considering transaction costs.
        slippage: 0.001 = 10bps (0.10%) per trade.
        """
        # Identify trades: absolute difference between today's position and yesterday's
        # Example: 0 to 1 = 1 trade, 1 to -1 = 2 trades (close long, open short)
        df['trades'] = df['position'].diff().abs().fillna(0)

        # Subtract costs from the log returns
        # We approximate: simple_ret - cost, then back to log
        # Or more simply: df['strategy_return'] - (df['trades'] * slippage)
        net_returns = df['strategy_return'] - (df['trades'] * slippage)

        returns = net_returns.dropna()
        cum_return = np.exp(returns.sum()) - 1
        ann_return = np.exp(returns.mean() * 252) - 1
        ann_vol = returns.std() * np.sqrt(252)
        sharpe = ann_return / ann_vol if ann_vol != 0 else 0

        cum_rets = np.exp(returns.cumsum())
        peak = cum_rets.cummax()
        max_dd = ((cum_rets - peak) / peak).min()

        return {
            "Total Return": f"{cum_return:.2%}",
            "Ann. Return": f"{ann_return:.2%}",
            "Max Drawdown": f"{max_dd:.2%}",
            "Sharpe Ratio": round(sharpe, 2),
            "Trade Count": int(df['trades'].sum())
        }

    def generate_report(self):
        """Generates a professional comparison tearsheet."""
        # 1. Intersect results with self.tickers + always include the benchmark
        active_results = {
            k: v for k, v in self.results.items()
            if k in self.tickers or k.startswith("BENCHMARK")
        }

        # 2. Convert to DataFrame for display
        summary_df = pd.DataFrame(active_results).T

        # Plotting
        plt.figure(figsize=(14, 7))
        sns.set_style("darkgrid")

        # Plot Strategy Equity Curves
        for ticker in self.tickers:
            if ticker in self.data:
                strat_cum = np.exp(self.data[ticker]['strategy_return'].cumsum())
                plt.plot(strat_cum, label=f"Strategy: {ticker} / {self.strat_name}", linewidth=2)

        # Plot Benchmark Equity Curve
        bench_cum = np.exp(self.benchmark_data['log_return'].cumsum())
        plt.plot(bench_cum, label=f"Benchmark: {self.benchmark_ticker}",
                 color='black', linestyle='--', alpha=0.7, linewidth=3)

        plt.title(f"Cumulative Performance vs {self.benchmark_ticker}", fontsize=16)
        plt.ylabel("Growth of $1")
        plt.legend()
        plt.show()

    def generate_portfolio_report(self):
        """Aggregates all tickers into one portfolio equity curve."""
        # 1. Pull the actual strategy return columns from self.data
        strat_rets_dict = {}
        for ticker in self.tickers:
            if 'strategy_return' in self.data[ticker].columns:
                strat_rets_dict[ticker] = self.data[ticker]['strategy_return']

        all_returns = pd.DataFrame(strat_rets_dict).fillna(0)

        # 2. Calculate Portfolio Returns (Equal Weighted)
        # We use mean across columns, then convert log returns to simple returns for the visual
        portfolio_log_returns = all_returns.mean(axis=1)
        portfolio_cum_growth = np.exp(portfolio_log_returns.cumsum())

        # 3. Portfolio Metrics
        port_total_ret = portfolio_cum_growth.iloc[-1] - 1
        port_ann_ret = np.exp(portfolio_log_returns.mean() * 252) - 1
        port_ann_vol = portfolio_log_returns.std() * np.sqrt(252)
        port_sharpe = port_ann_ret / port_ann_vol if port_ann_vol != 0 else 0

        print(f"\n=== AGGREGATE PORTFOLIO REPORT: {self.strat_name} ===")
        print(f"Total Tickers: {len(self.tickers)}")
        print(f"Portfolio Total Return: {port_total_ret:.2%}")
        print(f"Portfolio Sharpe Ratio: {port_sharpe:.2f}")

        # 4. Visualizing Portfolio vs Benchmark

        plt.figure(figsize=(14, 7))
        sns.set_style("whitegrid")

        plt.plot(portfolio_cum_growth, label='Total Portfolio (Equal Weighted)', color='gold', linewidth=3)

        bench_cum = np.exp(self.benchmark_data['log_return'].cumsum())
        plt.plot(bench_cum, label=f'Benchmark ({self.benchmark_ticker})', color='black', linestyle='--', alpha=0.6)

        plt.title(f"Portfolio Cumulative Performance - {self.strat_name}", fontsize=16)
        plt.ylabel("Growth of $1 (Log Scale Basis)")
        plt.legend()
        plt.show()

    def optimize_portfolio_selection(self, sharpe_threshold=0.3):
        """Returns a list of tickers that meet the quality threshold."""
        passed_tickers = []
        for ticker, metrics in self.results.items():
            if ticker.startswith("BENCHMARK"): continue

            # Check the raw float Sharpe we saved in the previous fix
            if metrics['Sharpe Ratio'] >= sharpe_threshold:
                passed_tickers.append(ticker)

        print(f"Optimization: Reduced portfolio from {len(self.tickers)} to {len(passed_tickers)} assets.")
        print(f"Re-run backtest and generate reports to see results.")

        self.tickers = passed_tickers
        return passed_tickers

    def generate_empty_grid(self, strategy_class):
        """
        Inspects a strategy class and returns a dictionary template
        for grid search parameters.
        """
        import inspect

        # Get the parameters of the __init__ method
        signature = inspect.signature(strategy_class.__init__)
        params = signature.parameters

        # Create a dictionary excluding 'self', 'args', and 'kwargs'
        grid_template = {
            name: None for name in params
            if name not in ['self', 'args', 'kwargs']
        }

        # print(f"--- Generated Grid Template for {strategy_class.__name__} ---")
        return grid_template

    def run_grid_search(self, strategy_class, param_grid):
      import itertools
      keys, values = zip(*param_grid.items())
      combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
      grid_results = []

      print(f"Starting Grid Search: {len(combinations)} combinations...")

      for params in combinations:
          strat = strategy_class(**params)
          run_returns = {} # Use a dict to keep track of ticker names

          for ticker in self.tickers:
              try:
                  df = self.data[ticker].copy()
                  df = strat.strat_apply(df)
                  df = df.dropna()

                  # Signal Shift & Return Calculation
                  df['position'] = df['signal'].shift(1).fillna(0)
                  df['log_return'] = np.log(df['close'] / df['close'].shift(1)).fillna(0)
                  # Store in dict with ticker as key
                  run_returns[ticker] = df['position'] * df['log_return']
              except Exception as e:
                  print(f"Error processing {ticker}: {e}")
                  continue

          if not run_returns:
              continue

          # FIX: Align all tickers by Date Index before taking the mean
          all_rets_df = pd.DataFrame(run_returns).fillna(0)
          portfolio_rets = all_rets_df.mean(axis=1)

          # Calculate Metrics (Dropping the first NaN from the shift)
          clean_rets = portfolio_rets.dropna()
          if len(clean_rets) > 0:
              ann_ret = np.exp(clean_rets.mean() * 252) - 1
              ann_vol = clean_rets.std() * np.sqrt(252)
              sharpe = ann_ret / ann_vol if ann_vol != 0 else 0
          else:
              ann_ret, sharpe = 0, 0

          grid_results.append({**params, 'Sharpe': sharpe, 'Return': ann_ret})

      return pd.DataFrame(grid_results)

    def plot_heatmap(self, grid_df, param_x, param_y, metric='Sharpe'):
        """Visualizes the grid search results to find stable plateaus."""
        pivot_table = grid_df.pivot(index=param_y, columns=param_x, values=metric)

        plt.figure(figsize=(10, 8))
        sns.heatmap(pivot_table, annot=True, fmt=".2f", cmap='RdYlGn', center=0)
        plt.title(f"Grid Search: {metric} Plateau ({param_x} vs {param_y})")
        plt.show()

"""## Strategy Classes"""

# Strategy Classes

class StrategyTemplate:
    """Base class to allow dynamic parameter passing for Grid Search."""
    def __init__(self, **params):
        self.params = params
        self.name = f"{self.__class__.__name__}_{params}"

    def strat_apply(self, df):
        raise NotImplementedError("Each strategy must implement strat_apply().")

class EMACross(StrategyTemplate):
    def strat_apply(self, df):
        # Access parameters from the params dictionary
        fast = int(self.params.get('fast', 10))
        slow = int(self.params.get('slow', 50))

        df['ema_fast'] = ta.ema(df['close'], length=fast)
        df['ema_slow'] = ta.ema(df['close'], length=slow)

        df['signal'] = np.nan
        long_condition = (
          df['ema_fast'] > df['ema_slow']
        )
        df.loc[long_condition, 'signal'] = 1
        short_condition = (
          df['ema_fast'] < df['ema_slow']
        )
        df.loc[short_condition, 'signal'] = -1
        df['signal'] = df['signal'].ffill().fillna(0)

        # # Not every strategy uses this, but this is the template if needed
        # flatten_condition = (
        #   (df['signal'] != 0) &
        #   ( cross_midline )
        # )

        # df['signal'] = df['signal'].mask(flatten_condition, 0)

        return df

class BollingerReversion(StrategyTemplate):
    def strat_apply(self, df):
        # 1. Parameter Extraction
        length = self.params.get('length', 20)
        std = self.params.get('std', 2.0)

        # 2. Indicator Calculation
        # ta.bbands returns a DataFrame with BBL, BBM, BBU, BBB, and BBP columns
        bbands = ta.bbands(df['close']
                           , length=length
                           , lower_std=std
                           , upper_std=std
                           , fillna=0.0)
        # print(bbands.head())

        # Accessing columns dynamically based on pandas-ta naming convention
        lower_band = bbands[f'BBL_{length}_{std}_{std}']
        upper_band = bbands[f'BBU_{length}_{std}_{std}']
        mid_band = bbands[f'BBM_{length}_{std}_{std}']

        # 3. Signal Logic
        df['signal'] = np.nan

        # Entry Logic
        df.loc[df['close'] < lower_band, 'signal'] = 1   # Long when below lower band
        df.loc[df['close'] > upper_band, 'signal'] = -1  # Short when above upper band

        # 4. Persistence (Holding positions)
        df['signal'] = df['signal'].ffill().fillna(0)

        # 5. Exit Logic (Flatten at Midline)
        # Vectorized crossover check: sign change in the difference
        diff = df['close'] - mid_band
        cross_midline = (diff * diff.shift(1) < 0) | (diff == 0)

        flatten_condition = (
            (df['signal'] != 0) &
            (cross_midline)
        )

        # Apply the exit condition to return to Cash (0)
        df['signal'] = df['signal'].mask(flatten_condition, 0)

        # Final forward fill to ensure the exit carries through until the next entry
        df['signal'] = df['signal'].ffill().fillna(0)

        return df

class RSIReversal(StrategyTemplate):
    def strat_apply(self, df):
        # 1. Parameter Extraction
        length = self.params.get('length', 14)
        lower_threshold = self.params.get('lower', 30)
        upper_threshold = self.params.get('upper', 70)
        midline = self.params.get('midline', 50)

        # 2. Indicator Calculation
        # Standardizing to lowercase 'rsi' and using the ta library syntax
        df['rsi'] = ta.rsi(df['close'], length=length)

        # 3. Signal Logic
        df['signal'] = np.nan

        # Entry Conditions
        df.loc[df['rsi'] < lower_threshold, 'signal'] = 1   # Long when oversold
        df.loc[df['rsi'] > upper_threshold, 'signal'] = -1  # Short when overbought

        # Initial Forward Fill to establish the directional bias
        df['signal'] = df['signal'].ffill().fillna(0)

        # 4. Exit Logic (Vectorized Crossover)
        # Check for when RSI crosses the midline (50) from either direction
        rsi_shifted = df['rsi'].shift(1)
        cross_midline = (
            ((rsi_shifted < midline) & (df['rsi'] >= midline)) | # Crossover
            ((rsi_shifted > midline) & (df['rsi'] <= midline))   # Crossunder
        )

        flatten_condition = (
            (df['signal'] != 0) &
            (cross_midline)
        )

        # Apply Exit
        df['signal'] = df['signal'].mask(flatten_condition, 0)

        # 5. Final Persistence
        # Re-apply ffill to ensure the '0' (Cash) state is held until a new entry trigger
        df['signal'] = df['signal'].ffill().fillna(0)

        return df

class Newsom10Strategy(StrategyTemplate):
    def strat_apply(self, df):
        # 1. Parameter Extraction
        atr_period = self.params.get('atr_period', 22)
        atr_mult = self.params.get('atr_mult', 3.0)
        bb_length = self.params.get('bb_length', 20)
        bb_mult = self.params.get('bb_mult', 2.0)
        ema_length = self.params.get('ema_length', 10)
        vol_ema_len = self.params.get('vol_ema_len', 20)

        # 2. Indicator Calculation
        df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=atr_period)
        df['ema_10'] = ta.ema(df['close'], length=ema_length)
        df['ohlc4'] = (df['open'] + df['high'] + df['low'] + df['close']) / 4

        # Volatility Filter
        df['atr_ratio'] = (df['atr'] / df['close']) * 100
        df['vol_filter_ema'] = ta.ema(df['atr_ratio'], length=vol_ema_len)
        df['vol_filter_active'] = df['atr_ratio'] > df['vol_filter_ema']

        # Bollinger Bands Expansion
        bbands = ta.bbands(df['close'], length=bb_length, std=bb_mult)
        df['bb_upper'] = bbands.iloc[:, 2]
        df['bb_lower'] = bbands.iloc[:, 0]
        df['band_dist'] = df['bb_upper'] - df['bb_lower']
        df['expansion'] = df['band_dist'] > df['band_dist'].shift(1)

        # 3. Chandelier Exit (Trailing Stop Logic)
        high_length = df['close'].rolling(window=atr_period).max()
        low_length = df['close'].rolling(window=atr_period).min()

        long_stop_raw = high_length - (df['atr'] * atr_mult)
        short_stop_raw = low_length + (df['atr'] * atr_mult)

        close_arr = df['close'].values
        ls_arr = long_stop_raw.fillna(0).values.copy()
        ss_arr = short_stop_raw.fillna(0).values.copy()
        directions = np.zeros(len(df))

        curr_dir = 1
        for i in range(1, len(df)):
            # Trailing Long Stop logic
            if close_arr[i-1] > ls_arr[i-1]:
                ls_arr[i] = max(ls_arr[i], ls_arr[i-1])

            # Trailing Short Stop logic
            if close_arr[i-1] < ss_arr[i-1]:
                ss_arr[i] = min(ss_arr[i], ss_arr[i-1])

            # Direction switch
            if close_arr[i] > ss_arr[i-1]:
                curr_dir = 1
            elif close_arr[i] < ls_arr[i-1]:
                curr_dir = -1
            directions[i] = curr_dir

        df['dir'] = directions

        # 4. Signal Logic
        df['signal'] = np.nan

        long_condition = (
            (df['close'] > df['ema_10']) &
            (df['close'].shift(1) < df['open'].shift(1)) &
            (df['ohlc4'] > df['ema_10']) &
            (df['dir'] == 1) &
            (df['expansion']) &
            (df['vol_filter_active'])
        )

        short_condition = (
            (df['close'] < df['ema_10']) &
            (df['close'].shift(1) > df['open'].shift(1)) &
            (df['ohlc4'] < df['ema_10']) &
            (df['dir'] == -1) &
            (df['expansion']) &
            (df['vol_filter_active'])
        )

        df.loc[long_condition, 'signal'] = 1
        df.loc[short_condition, 'signal'] = -1

        # 5. Persistence and Exit
        df['signal'] = df['signal'].ffill().fillna(0)

        # Flatten when Chandelier Direction changes
        flatten_condition = (
            (df['dir'] != df['dir'].shift(1)) &
            (df['signal'] != 0)
        )

        df['signal'] = df['signal'].mask(flatten_condition, 0)

        # Ensure position stays flat after mask until next entry signal
        df['signal'] = df['signal'].ffill().fillna(0)

        return df

"""## Instruments"""

crypto_crosswalk = pd.DataFrame([
    ("AAVEUSD", "AAVE-USD"),    ("ADAUSD", "ADA-USD"),    ("AIXBTUSD", "AIXBT-USD"),
    ("ALGOUSD", "ALGO-USD"),    ("APTUSD", "APT-USD"),    ("ARBUSD", "ARB-USD"),
    ("ATOMUSD", "ATOM-USD"),    ("AVAXUSD", "AVAX-USD"),
    ("BCHUSD", "BCH-USD"),    ("BNBUSD", "BNB-USD"),    ("BONKUSD", "BONK-USD"),
    ("BTCUSD", "BTC-USD"),    ("CRVUSD", "CRV-USD"),    ("DOGEUSD", "DOGE-USD"),
    ("DOTUSD", "DOT-USD"),    ("ETCUSD", "ETC-USD"),    ("ETHUSD", "ETH-USD"),
    ("FARTCOINUSD", "FARTCOIN-USD"),    ("FILUSD", "FIL-USD"),    ("FLOKIUSD", "FLOKI-USD"),
    ("HBARUSD", "HBAR-USD"),    ("HYPEUSD", "HYPE-USD"),
    ("INJUSD", "INJ-USD"),    ("IPUSD", "IP-USD"),    ("JTOUSD", "JTO-USD"),
    ("JUPUSD", "JUP-USD"),    ("KAITOUSD", "KAITO-USD"),    ("LDOUSD", "LDO-USD"),
    ("LINKUSD", "LINK-USD"),    ("LTCUSD", "LTC-USD"),    ("MOODENG", "MOODENG-USD"),
    ("NEARUSD", "NEAR-USD"),    ("ONDOUSD", "ONDO-USD"),    ("OPUSD", "OP-USD"),
    ("ORDIUSD", "ORDI-USD"),
    ("PNUTUSD", "PNUT-USD"),    ("POLUSD", "POL-USD"),
    ("RENDERUSD", "RENDER-USD"),    ("SUSD", "SUSD-USD"),
    ("SHIBUSD", "SHIB-USD"),    ("SOLUSD", "SOL-USD"),    ("STXUSD", "STX-USD"),
    ("SUIUSD", "SUI-USD"),    ("TIAUSD", "TIA-USD"),
    ("TONUSD", "TON-USD"),    ("TRUMPUSD", "TRUMP-USD"),    ("TRXUSD", "TRX-USD"),
    ("UNIUSD", "UNI-USD"),    ("VIRTUALUSD", "VIRTUAL-USD"),    ("WIFUSD", "WIF-USD"),
    ("WLDUSD", "WLD-USD"),    ("XPLUSD", "XPL-USD"),    ("XRPUSD", "XRP-USD"),
], columns=["breakout_symbol", "yfinance_symbol"])

crypto_assets = crypto_crosswalk['yfinance_symbol'].to_list()
# crypto_assets

forex_crosswalk = pd.DataFrame([
    # Major pairs
    ("EURUSD", "EURUSD=X"),    ("GBPUSD", "GBPUSD=X"),    ("USDJPY", "USDJPY=X"),
    ("USDCHF", "USDCHF=X"),    ("AUDUSD", "AUDUSD=X"),    ("USDCAD", "USDCAD=X"),
    ("NZDUSD", "NZDUSD=X"),

    # Minor (cross) pairs
    ("EURGBP", "EURGBP=X"),    ("EURJPY", "EURJPY=X"),    ("EURCHF", "EURCHF=X"),
    ("EURAUD", "EURAUD=X"),    ("EURCAD", "EURCAD=X"),    ("EURNZD", "EURNZD=X"),

    ("GBPJPY", "GBPJPY=X"),    ("GBPCHF", "GBPCHF=X"),    ("GBPAUD", "GBPAUD=X"),
    ("GBPCAD", "GBPCAD=X"),    ("GBPNZD", "GBPNZD=X"),

    ("AUDJPY", "AUDJPY=X"),    ("AUDCHF", "AUDCHF=X"),    ("AUDCAD", "AUDCAD=X"),
    ("AUDNZD", "AUDNZD=X"),

    ("CADJPY", "CADJPY=X"),    ("CADCHF", "CADCHF=X"),

    ("CHFJPY", "CHFJPY=X"),

    ("NZDJPY", "NZDJPY=X"),    ("NZDCHF", "NZDCHF=X"),    ("NZDCAD", "NZDCAD=X"),
], columns=["breakout_symbol", "yfinance_symbol"])

forex_assets = forex_crosswalk['yfinance_symbol'].to_list()
# forex_assets

instrument_type = "forex"

if instrument_type == "crypto":
    assets = crypto_assets
else:
    assets = forex_assets

"""## Run Backtests"""

try:
  # 1. Initialize
  engine = BacktestEngine(assets, start_date="2023-01-01", end_date="2025-12-01")

  # 2. Fetch
  engine.fetch_data()

  print("Data fetched for:", list(engine.data.keys()))
except Exception as e:
  print(f"Error processing assets: {e}")

# EMACross
# BollingerReversion
# RSIReversal
# Newsom10Strategy

# 2. Define ranges to test
param_grid_ema = {
    'fast': np.arange(5, 21, 2),
    'slow': np.arange(25, 100, 5)
}

param_grid_bb = {
    'length' : np.arange(25, 100, 5),
    'std' : np.linspace(1, 3, num=21)
}

param_grid_rsi = {
    'length' : np.arange(10, 21, 1),
    'lower_threshold' :  np.arange(25, 76, 5),
    'upper_threshold' :  np.arange(75, 101, 5),
    'midline' : np.arange(40, 61, 5)
}

param_grid_newsom10 = {
    'fast': np.arange(5, 21, 2),
    'slow': np.arange(25, 100, 5)
}

strats = [
    [EMACross, param_grid_ema],
    [BollingerReversion, param_grid_bb],
    [RSIReversal, param_grid_rsi],
    # [Newsom10Strategy, param_grid_newsom10]
    ]

# 3. Run Grid Search
grid_results = []
start_time = datetime.now()
for s in strats:
    gr = engine.run_grid_search(s[0], s[1])
    print(gr)
    grid_results.append(gr)
end_time = datetime.now()
print(f"Time Start: {start_time}")
print(f"Time End: {end_time}")
print(f"Time taken: {end_time - start_time}")

# 4. Visualize Results
param_sets = []
for gr in grid_results:
  if len(gr.columns) == 4 and 'fast' in gr.columns:
    engine.plot_heatmap(gr, param_x='fast', param_y='slow', metric='Sharpe')
  elif len(gr.columns) == 4 and 'length' in gr.columns:
      engine.plot_heatmap(gr, param_x='length', param_y='std', metric='Sharpe')
  else:
    analyze_complex_grid(gr)

  # Identify best params
  best_params = gr.loc[gr['Sharpe'].idxmax()]
  print(f"\nBest Parameters Found:\n{best_params}")

  param_sets.append(best_params)

param_sets, strats

# 1. Choose the best parameters from the grid
for idx, ((s, _), ps) in enumerate(zip( strats, param_sets)):
  best_params = ps.to_dict()
  # Remove metrics from dict to leave only strategy params
  best_params.pop('Sharpe', None)
  best_params.pop('Return', None)

  print(best_params)

  # 2. Run the engine with these settings
  final_strat = s(**best_params)
  engine.run_strategy(final_strat, name=f"Optimized_{idx}")

  # 3. Quality Control: Keep only tickers with Sharpe > 0.4
  high_quality_tickers = engine.optimize_portfolio_selection(sharpe_threshold=0.3)

  # 4. Final Performance Report for the curated group
  engine.generate_portfolio_report()